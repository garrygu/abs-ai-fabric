name: abs-core

networks:
  abs-net:
    name: ${ABS_NETWORK}
    driver: bridge

volumes:
  ollama_models:
  qdrant_storage:
  redis_data:
  onyx_data:

services:

  # ---- LLM Runtime (choose ONE) ----
  # A) OLLAMA (default): simple, offline, great for workstation
  ollama:
    image: ollama/ollama:0.3.8
    container_name: abs-ollama
    restart: unless-stopped
    runtime: nvidia
    environment:
      TZ: ${TZ}
      OLLAMA_KEEP_ALIVE: "24h"
    volumes:
      - ollama_models:/root/.ollama
    ports:
      - "${OLLAMA_PORT}:11434"
    networks: [abs-net]
    healthcheck:
      test: ["CMD", "wget", "-q", "-O", "-", "http://localhost:11434/api/tags"]
      interval: 10s
      timeout: 3s
      retries: 30
    # If you plan to use vLLM instead, comment this service out and enable vllm below.

  # B) vLLM (OpenAI-compatible). Use instead of Ollama if you want /v1/chat/completions API.
  # vllm:
  #   image: vllm/vllm-openai:0.5.4
  #   container_name: abs-vllm
  #   restart: unless-stopped
  #   runtime: nvidia
  #   environment:
  #     TZ: ${TZ}
  #   command: >
  #     --model ${VLLM_MODEL}
  #     --gpu-memory-utilization 0.90
  #     --dtype float16
  #     --max-model-len 8192
  #   ports:
  #     - "${VLLM_PORT}:8000"
  #   networks: [abs-net]
  #   healthcheck:
  #     test: ["CMD", "wget", "-q", "-O", "-", "http://localhost:8000/v1/models"]
  #     interval: 10s
  #     timeout: 3s
  #     retries: 40

  # ---- Vector DB ----
  qdrant:
    image: qdrant/qdrant:v1.9.2
    container_name: abs-qdrant
    restart: unless-stopped
    environment:
      TZ: ${TZ}
    volumes:
      - qdrant_storage:/qdrant/storage
    ports:
      - "6333:6333"   # REST
      - "6334:6334"   # gRPC
    networks: [abs-net]
    healthcheck:
      test: ["CMD", "wget", "-q", "-O", "-", "http://localhost:6333/collections"]
      interval: 10s
      timeout: 3s
      retries: 20

  # ---- Cache / Queue ----
  redis:
    image: redis:7.2
    container_name: abs-redis
    restart: unless-stopped
    command: ["redis-server", "--appendonly", "yes"]
    volumes:
      - redis_data:/data
    ports:
      - "6379:6379"
    networks: [abs-net]
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 2s
      retries: 15

  # ---- RAG/Agent Engine ----
  onyx:
    build: ./onyx
    container_name: abs-onyx
    restart: unless-stopped
    environment:
      TZ: ${TZ}
      # Connect to inference engines
      OLLAMA_BASE_URL: http://ollama:11434
      OPENAI_BASE_URL: http://vllm:8000/v1
      OPENAI_API_KEY: abs-local
      # Connect to vector database
      QDRANT_URL: http://qdrant:6333
      QDRANT_API_KEY: ""
      # Connect to cache
      REDIS_URL: redis://redis:6379/0
      # Onyx-specific configuration
      ONYX_PORT: 8000
      ONYX_LOG_LEVEL: info
    volumes:
      - onyx_data:/app/data
    ports:
      - "8000:8000"
    networks: [abs-net]
    depends_on:
      - redis
      - qdrant
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 5

  # ---- Hub Gateway ----
  hub-gateway:
    build: ./gateway
    container_name: abs-hub-gateway
    environment:
      REDIS_URL: redis://redis:6379/0
      OLLAMA_BASE_URL: http://ollama:11434
      OPENAI_BASE_URL: http://vllm:8000/v1
      OPENAI_API_KEY: abs-local
      ONYX_BASE_URL: http://onyx:8000
      REGISTRY_PATH: /app/registry.json
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    ports: ["8081:8081"]
    depends_on: [redis, onyx]
    networks: [abs-net]